{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "In [chapter 3](https://learn.deeplearning.ai/courses/langchain/lesson/3/memory) the topic was Memory, i.e. how to make LLMs remember information. LLMs are inherently stateless, i.e. they don't remember information from one call to the next. Therefore, we need to pass the previous conversation as context to the LLM to create a chat-like experience.\n",
    "\n",
    "In my previous blog post on [building chat from scratch](https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/), I implemented a chat messages class to store the conversation history. A chat client class handled the conversation to the LLM.\n",
    "\n",
    "Following the lesson, let's first re-create the chat experience using langchain.\n",
    "\n",
    "Afterwards, I will implement a use case including conversation summarization. Even though, the context sizes have increased massively over the past year, and cost per token has significantly declined, I still find the approach interesting, and it should be a good learning exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with langchain\n",
    "\n",
    "Let's start by importing the necessary modules and initializing the LLM and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Initialize the memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add system message to the memory\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant. Please provide concise answers.\")\n",
    "memory.chat_memory.add_message(system_message)\n",
    "\n",
    "# Initialize the conversation\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False # Set to True to see the prompt and response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant. Please provide concise answers.\n",
      "Human: Please list the planets in the solar system\n",
      "AI: Sure! The planets in our solar system, in order from the Sun, are:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "\n",
      "There are also dwarf planets, like Pluto, but those are not classified as the main planets. Let me know if you want more information about any of them!\n"
     ]
    }
   ],
   "source": [
    "# add system prompt\n",
    "\n",
    "\n",
    "#prompt = \"Hi, my name is Christian\"\n",
    "prompt = \"Please list the planets in the solar system\"\n",
    "model_response = conversation.predict(input=prompt)\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant. Please provide concise answers.\n",
      "Human: Please list the planets in the solar system\n",
      "AI: Sure! The planets in our solar system, in order from the Sun, are:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "\n",
      "There are also dwarf planets, like Pluto, but those are not classified as the main planets. Let me know if you want more information about any of them!\n",
      "Human: Please reverse the list\n",
      "AI: Of course! Hereâ€™s the list of planets in reverse order:\n",
      "\n",
      "1. Neptune\n",
      "2. Uranus\n",
      "3. Saturn\n",
      "4. Jupiter\n",
      "5. Mars\n",
      "6. Earth\n",
      "7. Venus\n",
      "8. Mercury\n",
      "\n",
      "If you need anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please reverse the list\"\n",
    "model_response = conversation.predict(input=prompt)\n",
    "print(memory.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned in the lesson, we can also inject previous messages / a conversation history into the LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Initialize the memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation.memory = memory\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: That sounds nice! Sometimes just taking it easy is the best way to spend your time. Do you have any plans for the day, or are you just going with the flow?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Not much, just hanging\"\n",
    "model_response = conversation.predict(input=prompt)\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Summarization\n",
    "\n",
    "The lesson introduced different types of memory. I am not going to repeat all the examples, but I will show how to implement the conversation summarization use case.\n",
    "\n",
    "For conversation summarization, we will use the [Daily Dialog](https://huggingface.co/datasets/li2017dailydialog/daily_dialog) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the token from the environment variable\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/chrwittm/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('daily_dialog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialog', 'act', 'emotion'],\n",
       "        num_rows: 11118\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['dialog', 'act', 'emotion'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dialog', 'act', 'emotion'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains more than 10k conversations. \n",
    "\n",
    "Let's take a look at one conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Say , Jim , how about going for a few beers after dinner ? ',\n",
       " ' You know that is tempting but is really not good for our fitness . ',\n",
       " ' What do you mean ? It will help us to relax . ',\n",
       " \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n",
       " \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n",
       " ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n",
       " \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n",
       " ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n",
       " \" Good.Let ' s go now . \",\n",
       " ' All right . ']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "dialog = dataset['train'][index]['dialog']\n",
    "dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More let's move the dialog into the langchain memory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the conversation to the memory and print the buffer to see what is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human suggests going for a few beers after dinner, but the AI declines, stating that it is tempting but not good for their fitness. The human argues that it would help them relax, but the AI disagrees, recalling that it would just lead to negative consequences like weight gain and silliness, referencing a previous experience. The human concedes the AI's point but expresses a desire to do something other than stay at home. The AI then suggests a walk to the gym to play singsong and meet friends, to which the human agrees, mentioning that Mary and Sally often go there to play ping pong and suggesting they could make a foursome with them. The AI responds positively, suggesting that if Mary and Sally are willing, they could also ask them to go dancing, which the AI considers excellent exercise and fun.\n",
      "Human:  Good.Let ' s go now . \n",
      "AI:  All right . \n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "for i in range(0, len(dialog),2):\n",
    "    memory.save_context({\"input\": dialog[i]}, {\"output\": dialog[i+1]})\n",
    "\n",
    "print(memory.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the stores the summarized conversation. Let's continue the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really glad you like it! It's always nice to mix a bit of exercise with socializing. Plus, the fresh air will do us good! Do you have any favorite songs in mind for our singsong session? I think it would be fun to belt out some classics or maybe even some upbeat tunes to get us in the mood for dancing later!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the conversation with the memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "prompt = \"I am glad you came up with this idea\"\n",
    "model_response = conversation.predict(input=prompt)\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of the conversation has been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human suggests going for a few beers after dinner, but the AI declines, stating that it is tempting but not good for their fitness. The human argues that it would help them relax, but the AI disagrees, recalling that it would lead to negative consequences like weight gain and silly behavior. The human concedes but expresses a desire to do something other than stay at home. The AI then suggests a walk to the gym to play singsong and meet friends, to which the human agrees, mentioning that Mary and Sally often go there to play ping pong and suggesting they could make a foursome with them. The AI proposes that if Mary and Sally are willing, they could also ask them to go dancing, which the AI considers excellent exercise and fun. The human agrees to go now, and the AI expresses happiness that the human liked the idea, emphasizing the benefits of fresh air and catching up with friends. The AI also asks if the human has a favorite song to sing during singsong, suggesting it would be fun to sing classics together.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
