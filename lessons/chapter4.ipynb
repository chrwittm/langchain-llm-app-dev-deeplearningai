{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Chains\n",
    "\n",
    "https://learn.deeplearning.ai/courses/langchain/lesson/4/chains\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() #contains the OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple chain\n",
    "\n",
    "Let's implement the simplest possible chain. Using my go-to hello world-prompt, asking the LM to list the planets of the solar system, we can construct a static chain. For running the chain, we need to pass an empty dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The planets in our solar system, in order from the Sun, are:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "\n",
      "Additionally, there are dwarf planets, such as Pluto, Eris, Haumea, and Makemake, which are also part of our solar system but are not classified as full-fledged planets.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Initialize the prompt\n",
    "prompt = PromptTemplate.from_template(\"List the planets in the solar system\")\n",
    "\n",
    "# Initialize the static chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run({})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, chains are designed to be dynamic, so let's use a prompt template with a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response = chain.run({\"country\": \"France\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Sequential chain\n",
    "\n",
    "For implementing a sequential chain, we will use the Wittmann-Tours blogposts like in chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_blog_post_files(path_to_blog):\n",
    "\n",
    "    pattern = os.path.join(path_to_blog, \"**/*.md\")\n",
    "    return sorted(glob.glob(pattern, recursive=True))\n",
    "\n",
    "def get_blogpost(path_to_blogpost):\n",
    "    with open(path_to_blogpost, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blogpost has 7666 characters. \n",
      "\n",
      "Heading and first sentence: \n",
      "\n",
      "# 3 Tage in Melbourne\n",
      "\n",
      "Auch wenn Canberra die offizielle Hauptstadt Australiens ist, so liefern sich Melbourne und Sydney als die beiden größten Städte des Kontinents ein Wettrennen um die Wahrnehmung als geistige Kapitale des Landes. Nach relativ viel Naturprogramm besuchten wir Melbourne, „[the world's most liveable city](https://www.smh.com.au/business/the-economy/melbourne-named-worlds-most-liveable-city-by-the-economist-for-seventh-year-20170816-gxx1kg.html)“, zu der sie der Economist wiederholt gekürt hat. \n"
     ]
    }
   ],
   "source": [
    "path_to_blogpost = \"./../wt-blogposts/3-tage-in-melbourne/index.md\"\n",
    "blogpost = get_blogpost(path_to_blogpost)\n",
    "\n",
    "print(f\"The blogpost has {len(blogpost)} characters. \\n\")\n",
    "print(f\"Heading and first sentence: \\n\\n{blogpost[317:835]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "El blog detalla una visita de tres días a Melbourne, Australia, destacando su estatus como un centro cultural a pesar de que Canberra sea la capital oficial. El autor aprecia la arquitectura de Melbourne, particularmente la histórica Estación Flinders Street, y sus vibrantes arcadas comerciales. La visita coincidió con el Abierto de Australia, creando una atmósfera animada en toda la ciudad. El post también explora la escena del arte callejero de Melbourne y una visita al Museo de Cine ACMI, que exhibe animaciones de Aardman. Además, el autor aprende sobre la figura legendaria de Ned Kelly en la Biblioteca Estatal de Victoria. En general, la experiencia en Melbourne se describe como enriquecedora y placentera.\n"
     ]
    }
   ],
   "source": [
    "#First Chain\n",
    "prompt1 = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Summarize the following blog post delimited by triple backticks into {summary_words} words.\n",
    "    Blog post:\n",
    "    ```{blogpost}```\n",
    "    \"\"\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"summarized_blog_post\")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Translate the {summarized_blog_post} into {language}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"translated_blog_post_summary\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"blogpost\", \"summary_words\", \"language\"],\n",
    "    output_variables=[\"translated_blog_post_summary\"],\n",
    "    verbose=True)\n",
    "\n",
    "response = overall_chain.run(blogpost=blogpost, summary_words=100, language=\"Spanish\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output only contains the translated summary. We can use the `.batch()` method to get the summarized blog post as well.\n",
    "\n",
    "Notice that I adjusted the prompt to return the summary in the original language. Without this instruction, the LLM would return the summary in the language of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarized Blog Post: Der Blogbeitrag beschreibt einen dreitägigen Aufenthalt in Melbourne, der als \"lebenswerteste Stadt der Welt\" gilt. Die Autoren erkunden die Stadt, die für ihre beeindruckende Architektur, Kunst und Kultur bekannt ist. Sie besuchen die Flinders Street Station, die Royal Arcade und erleben das Tennisfieber während der Australian Open. Streetart in Gassen wie Hosier Lane wird hervorgehoben, ebenso wie ein Besuch im ACMI-Film Museum, wo sie mehr über Aardman-Produktionen erfahren. Zudem lernen sie die Legende von Ned Kelly in der Victoria State Library kennen. Insgesamt genießen sie die Vielfalt und den Charme Melbournes. \n",
      "\n",
      "Translated Blog Post Summary: El blog describe una estancia de tres días en Melbourne, que es considerada la \"ciudad más habitable del mundo\". Los autores exploran la ciudad, conocida por su impresionante arquitectura, arte y cultura. Visitan la estación Flinders Street, la Royal Arcade y experimentan la fiebre del tenis durante el Abierto de Australia. Se destaca el arte callejero en callejones como Hosier Lane, así como una visita al Museo de Cine ACMI, donde aprenden más sobre las producciones de Aardman. Además, conocen la leyenda de Ned Kelly en la Biblioteca Estatal de Victoria. En general, disfrutan de la diversidad y el encanto de Melbourne.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Summarize the following blog post delimited by triple backticks into {summary_words} words in its original language.\n",
    "    Blog post:\n",
    "    ```{blogpost}```\n",
    "    \"\"\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"summarized_blog_post\")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Translate the {summarized_blog_post} into {language}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"translated_blog_post_summary\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"blogpost\", \"summary_words\", \"language\"],\n",
    "    # Request both outputs\n",
    "    output_variables=[\"summarized_blog_post\", \"translated_blog_post_summary\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use `.batch()` to run the chain and get multiple outputs\n",
    "response = overall_chain.batch([{\n",
    "    \"blogpost\": blogpost, \n",
    "    \"summary_words\": 100, \n",
    "    \"language\": \"Spanish\"\n",
    "}])\n",
    "\n",
    "# Output both the summarized blog post and the translated version\n",
    "print(\"Summarized Blog Post:\", response[0][\"summarized_blog_post\"], \"\\n\")\n",
    "print(\"Translated Blog Post Summary:\", response[0][\"translated_blog_post_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router chain\n",
    "\n",
    "Since LLMs have become so good at answering questions from multiple domains, let's deviate from the example of lesson 4. Let's to pretend that the LLM has access to different API's. Using the router chain, we will determine if we need to call an API or if this is a generic question for the large language model.\n",
    "\n",
    "Use Case: You have multiple APIs that serve different purposes (e.g., weather, stock prices, and news). Based on the user’s request, the router chain can determine which API to call.\n",
    "Example: If the user asks, \"What’s the current weather in New York?\" the router chain sends the request to a weather API, while \"Show me the latest news headlines\" would be routed to a news API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_api_prompt = \"\"\"You are an LLM which uses your hallucination power to mock an API call to a weather service.\n",
    "    Create a convincing response in plain text as if you were the weather service.\n",
    "    Here is the user request:\n",
    "    {input}\n",
    "\"\"\"\n",
    "\n",
    "stock_api_prompt = \"\"\"You are an LLM which uses your hallucination power to mock an API call to a stock service.\n",
    "    Create a convincing response in plain text as if you were the stock service.\n",
    "    Here is the user request:\n",
    "    {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"weather\", \n",
    "        \"description\": \"API to get weather data\", \n",
    "        \"prompt_template\": weather_api_prompt\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stock\", \n",
    "        \"description\": \"API to get stock data\", \n",
    "        \"prompt_template\": stock_api_prompt\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the prompt templates into chains. Additionally, we need to format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "destination_chains = {}\n",
    "for prompt_info in prompt_infos:\n",
    "    name = prompt_info[\"name\"]\n",
    "    prompt_template = prompt_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weather': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are an LLM which uses your hallucination power to mock an API call to a weather service.\\n    Create a convincing response in plain text as if you were the weather service.\\n    Here is the user request:\\n    {input}\\n'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10f80de40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10f815e70>, root_client=<openai.OpenAI object at 0x10f803490>, root_async_client=<openai.AsyncOpenAI object at 0x10f80cc70>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')),\n",
       " 'stock': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are an LLM which uses your hallucination power to mock an API call to a stock service.\\n    Create a convincing response in plain text as if you were the stock service.\\n    Here is the user request:\\n    {input}\\n'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10f80de40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10f815e70>, root_client=<openai.OpenAI object at 0x10f803490>, root_async_client=<openai.AsyncOpenAI object at 0x10f80cc70>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''))}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather: API to get weather data\n",
      "stock: API to get stock data\n"
     ]
    }
   ],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model, select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "weather: {'input': 'What is the weather in New York?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Weather Report for New York City**\n",
      "\n",
      "**Date:** October 5, 2023  \n",
      "**Time:** 10:00 AM EDT\n",
      "\n",
      "**Current Conditions:**  \n",
      "- **Temperature:** 68°F (20°C)  \n",
      "- **Humidity:** 60%  \n",
      "- **Wind:** 10 mph from the NW  \n",
      "- **Conditions:** Partly cloudy with occasional sunshine  \n",
      "\n",
      "**Forecast:**  \n",
      "- **Today:** Expect a high of 72°F (22°C) with a mix of sun and clouds. A slight chance of isolated showers in the late afternoon.  \n",
      "- **Tonight:** Temperatures will drop to around 58°F (14°C) with mostly clear skies.  \n",
      "\n",
      "**Extended Forecast:**  \n",
      "- **Saturday:** Sunny with a high of 75°F (24°C).  \n",
      "- **Sunday:** Partly cloudy, high near 70°F (21°C).  \n",
      "- **Monday:** Chance of rain, high of 66°F (19°C).  \n",
      "\n",
      "**Advisory:** No significant weather advisories are in effect at this time. Enjoy your day!\n",
      "\n",
      "For more updates, please check back later or visit our website.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is the weather in New York?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "stock: {'input': 'What is the current stock price of Nvidia?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As of the latest update, the current stock price of Nvidia (NVDA) is $450.25. Please note that stock prices are subject to change and may vary throughout the trading day. For the most accurate and up-to-date information, please check a reliable financial news source or stock market platform.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is the stock price of Nvidia?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical object that absorbs all incident radiation, regardless of frequency or angle of incidence. A black body is also a perfect emitter of radiation, meaning it emits energy at all wavelengths.\n",
      "\n",
      "The characteristics of black body radiation are described by Planck's law, which states that the intensity of radiation emitted by a black body at a given temperature is a function of wavelength. Key points about black body radiation include:\n",
      "\n",
      "1. **Temperature Dependence**: The amount and spectrum of radiation emitted by a black body depend on its temperature. As the temperature increases, the total energy emitted increases, and the peak wavelength of the emitted radiation shifts to shorter wavelengths (Wien's displacement law).\n",
      "\n",
      "2. **Spectrum**: The radiation emitted by a black body is continuous and covers a wide range of wavelengths. At lower temperatures, the radiation is primarily in the infrared range, while at higher temperatures, it can include visible light and even ultraviolet radiation.\n",
      "\n",
      "3. **Stefan-Boltzmann Law**: The total energy emitted per unit surface area of a black body is proportional to the fourth power of its absolute temperature (T). This is expressed mathematically as \\( E = \\sigma T^4 \\), where \\( E \\) is the total energy emitted, \\( T \\) is the temperature in Kelvin, and \\( \\sigma \\) is the Stefan-Boltzmann constant.\n",
      "\n",
      "4. **Quantum Mechanics**: The study of black body radiation played a crucial role in the development of quantum mechanics. The classical physics approach could not explain the observed spectrum of radiation emitted by black bodies, leading to the \"ultraviolet catastrophe.\" Max Planck resolved this issue by introducing the concept of quantized energy levels, which laid the groundwork for quantum theory.\n",
      "\n",
      "Black body radiation is a fundamental concept in physics and has applications in various fields, including thermodynamics, astrophysics, and materials science. It helps in understanding phenomena such as the temperature of stars and the behavior of materials at different temperatures.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is black body radiation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Router Chain\n",
    "\n",
    "After having recreated the example from the lesson, I feel that the long router prompt template is a risks because it is quite lengthy. Therefore, I tried to simplify the router prompt template, by using response schemas from [chapter2](chapter2.ipynb).\n",
    "\n",
    "This re-factoring, however, was surprisingly difficult because I had to do other things manually which have been done automatically before.\n",
    "\n",
    "- The `destination` needed to be empty to fall back to the default schema\n",
    "- The `next_inputs` needed to be a dictionary with key field `input`.\n",
    "- The curly braces in the output JSON needed to be manually escaped, otherwise this would cause subsequent errors, because `\\n\\t\"destination\"` was mapped to an input field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{{\n",
      "\t\"destination\": string  // name of the prompt to use (leave empty or invalid if no prompt matches)\n",
      "\t\"next_inputs\": string  // A dictionary with the key 'input' containing the modified or original user input\n",
      "}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "#destination_schema = ResponseSchema(name=\"destination\", description=\"name of the prompt to use or `DEFAULT`\")\n",
    "destination_schema = ResponseSchema(name=\"destination\", description=\"name of the prompt to use (leave empty or invalid if no prompt matches)\")\n",
    "\n",
    "next_inputs_schema = ResponseSchema(\n",
    "    name=\"next_inputs\", \n",
    "    description=\"A dictionary with the key 'input' containing the modified or original user input\" )\n",
    "\n",
    "response_schema = [destination_schema, next_inputs_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "formatting_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Manually escape curly braces for use in PromptTemplate\n",
    "formatting_instructions = formatting_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "\n",
    "print(formatting_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather: API to get weather data\n",
      "stock: API to get stock data\n"
     ]
    }
   ],
   "source": [
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt indeed is now a lot simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Evaluate the following input to select the best model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a description of what the prompt is best suited for.\n",
    "\n",
    "<< FORMATTING >>\n",
    "{formatting_instructions}\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str,\n",
    "    formatting_instructions=formatting_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the following input to select the best model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for.\n",
      "\n",
      "<< FORMATTING >>\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{{\n",
      "\t\"destination\": string  // name of the prompt to use (leave empty or invalid if no prompt matches)\n",
      "\t\"next_inputs\": string  // A dictionary with the key 'input' containing the modified or original user input\n",
      "}}\n",
      "```\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "weather: API to get weather data\n",
      "stock: API to get stock data\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct PromptTemplate setup\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,  # The correctly formatted template\n",
    "    input_variables=[\"input\"],  # The input from the user\n",
    "    output_parser=output_parser  # Parse the output, but destination is not an input variable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] output_parser=StructuredOutputParser(response_schemas=[ResponseSchema(name='destination', description='name of the prompt to use (leave empty or invalid if no prompt matches)', type='string'), ResponseSchema(name='next_inputs', description=\"A dictionary with the key 'input' containing the modified or original user input\", type='string')]) template='Evaluate the following input to select the best model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for.\\n\\n<< FORMATTING >>\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{{\\n\\t\"destination\": string  // name of the prompt to use (leave empty or invalid if no prompt matches)\\n\\t\"next_inputs\": string  // A dictionary with the key \\'input\\' containing the modified or original user input\\n}}\\n```\\n\\n<< CANDIDATE PROMPTS >>\\nweather: API to get weather data\\nstock: API to get stock data\\n\\n<< INPUT >>\\n{input}'\n"
     ]
    }
   ],
   "source": [
    "router_prompt.input_variables=['input']\n",
    "print(router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]) llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10f80de40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10f815e70>, root_client=<openai.OpenAI object at 0x10f803490>, root_async_client=<openai.AsyncOpenAI object at 0x10f80cc70>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "print(default_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "weather: {'input': 'What is the weather in New York?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Weather Report for New York City**\n",
      "\n",
      "**Date:** October 5, 2023  \n",
      "**Location:** New York, NY\n",
      "\n",
      "**Current Conditions:**  \n",
      "- **Temperature:** 68°F (20°C)  \n",
      "- **Humidity:** 60%  \n",
      "- **Wind:** 10 mph from the NW  \n",
      "- **Conditions:** Partly cloudy with occasional sunshine\n",
      "\n",
      "**Forecast:**  \n",
      "- **Today:** Expect a high of 72°F (22°C) and a low of 58°F (14°C). A mix of sun and clouds throughout the day, with a slight chance of isolated showers in the late afternoon.  \n",
      "- **Tonight:** Clear skies with temperatures dropping to around 60°F (16°C). Light winds will continue.\n",
      "\n",
      "**Extended Forecast:**  \n",
      "- **Saturday:** Mostly sunny, high of 75°F (24°C).  \n",
      "- **Sunday:** Partly cloudy, high of 70°F (21°C), with a chance of rain in the evening.  \n",
      "- **Monday:** Cloudy with scattered thunderstorms, high of 68°F (20°C).\n",
      "\n",
      "**Advisory:** No significant weather advisories are in effect at this time. Enjoy your day!\n",
      "\n",
      "For more updates, please check back later or visit our website.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(input=\"What is the weather in New York?\", destinations=destinations_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "stock: {'input': 'What is the stock price of Nvidia?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As of the latest market update, the stock price of Nvidia (NVDA) is $450.25. Please note that stock prices are subject to change and may vary throughout the trading day. For real-time updates, please check a financial news website or your brokerage platform.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is the stock price of Nvidia?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      ": {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical object that absorbs all incident radiation, regardless of frequency or angle of incidence. A black body is also a perfect emitter of radiation, meaning it emits energy at all wavelengths.\n",
      "\n",
      "The characteristics of black body radiation are described by Planck's law, which states that the intensity of radiation emitted by a black body at a given temperature is a function of wavelength. Key points about black body radiation include:\n",
      "\n",
      "1. **Temperature Dependence**: The amount and distribution of radiation emitted by a black body depend on its temperature. As the temperature increases, the total energy emitted increases, and the peak wavelength of the emitted radiation shifts to shorter wavelengths (Wien's displacement law).\n",
      "\n",
      "2. **Spectrum**: The spectrum of black body radiation is continuous and covers a wide range of wavelengths, from infrared to ultraviolet. At lower temperatures, the radiation is primarily in the infrared range, while at higher temperatures, it can include visible light.\n",
      "\n",
      "3. **Stefan-Boltzmann Law**: The total energy emitted per unit surface area of a black body is proportional to the fourth power of its absolute temperature (T). This is expressed mathematically as \\( E = \\sigma T^4 \\), where \\( E \\) is the total energy emitted, \\( T \\) is the temperature in Kelvin, and \\( \\sigma \\) is the Stefan-Boltzmann constant.\n",
      "\n",
      "4. **Quantum Theory**: The study of black body radiation played a crucial role in the development of quantum mechanics. The ultraviolet catastrophe, which arose from classical physics predictions that suggested an infinite amount of energy would be emitted at short wavelengths, was resolved by Max Planck's introduction of quantized energy levels, leading to the formulation of Planck's law.\n",
      "\n",
      "Black body radiation is fundamental in various fields, including thermodynamics, astrophysics, and quantum mechanics, and it provides a basis for understanding the emission and absorption of radiation by real materials.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is black body radiation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "Chaining the operations together takes some time getting used to, but re-implementing the example from chapter 4 with different use cases was very educational."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
