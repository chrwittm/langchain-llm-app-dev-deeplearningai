{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Chains\n",
    "\n",
    "https://learn.deeplearning.ai/courses/langchain/lesson/4/chains\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() #contains the OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple chain\n",
    "\n",
    "Let's implement the simplest possible chain. Using my go-to hello world-prompt, asking the LM to list the planets of the solar system, we can construct a static chain. For running the chain, we need to pass an empty dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrwittm/miniforge3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/Users/chrwittm/miniforge3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The planets in our solar system, in order from the Sun, are:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "\n",
      "Additionally, there are dwarf planets, such as Pluto, Eris, Haumea, and Makemake, which are also part of our solar system but are not classified as full-fledged planets.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Initialize the prompt\n",
    "prompt = PromptTemplate.from_template(\"List the planets in the solar system\")\n",
    "\n",
    "# Initialize the static chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run({})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, chains are designed to be dynamic, so let's use a prompt template with a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response = chain.run({\"country\": \"France\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Sequential chain\n",
    "\n",
    "For implementing a sequential chain, we will use the Wittmann-Tours blogposts like in chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_blog_post_files(path_to_blog):\n",
    "\n",
    "    pattern = os.path.join(path_to_blog, \"**/*.md\")\n",
    "    return sorted(glob.glob(pattern, recursive=True))\n",
    "\n",
    "def get_blogpost(path_to_blogpost):\n",
    "    with open(path_to_blogpost, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blogpost has 7666 characters. \n",
      "\n",
      "Heading and first sentence: \n",
      "\n",
      "# 3 Tage in Melbourne\n",
      "\n",
      "Auch wenn Canberra die offizielle Hauptstadt Australiens ist, so liefern sich Melbourne und Sydney als die beiden größten Städte des Kontinents ein Wettrennen um die Wahrnehmung als geistige Kapitale des Landes. Nach relativ viel Naturprogramm besuchten wir Melbourne, „[the world's most liveable city](https://www.smh.com.au/business/the-economy/melbourne-named-worlds-most-liveable-city-by-the-economist-for-seventh-year-20170816-gxx1kg.html)“, zu der sie der Economist wiederholt gekürt hat. \n"
     ]
    }
   ],
   "source": [
    "path_to_blogpost = \"./../wt-blogposts/3-tage-in-melbourne/index.md\"\n",
    "blogpost = get_blogpost(path_to_blogpost)\n",
    "\n",
    "print(f\"The blogpost has {len(blogpost)} characters. \\n\")\n",
    "print(f\"Heading and first sentence: \\n\\n{blogpost[317:835]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "El blog detalla una visita de tres días a Melbourne, Australia, destacando su estatus como un vibrante centro cultural, que a menudo compite con Sídney por reconocimiento. El autor describe la impresionante arquitectura de Melbourne, particularmente la histórica Estación Flinders Street, y sus encantadoras arcadas llenas de tiendas únicas. La visita coincidió con el Abierto de Australia, creando una atmósfera animada en toda la ciudad. La publicación también enfatiza la escena del arte callejero de Melbourne y una visita al Museo de Cine ACMI, que exhibe animaciones de Aardman. Además, el autor reflexiona sobre la historia de la ciudad, incluyendo la leyenda de Ned Kelly, y expresa su gratitud por la ayuda local durante su estancia.\n"
     ]
    }
   ],
   "source": [
    "#First Chain\n",
    "prompt1 = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Summarize the following blog post delimited by triple backticks into {summary_words} words.\n",
    "    Blog post:\n",
    "    ```{blogpost}```\n",
    "    \"\"\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"summarized_blog_post\")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Translate the {summarized_blog_post} into {language}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"translated_blog_post_summary\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"blogpost\", \"summary_words\", \"language\"],\n",
    "    output_variables=[\"translated_blog_post_summary\"],\n",
    "    verbose=True)\n",
    "\n",
    "response = overall_chain.run(blogpost=blogpost, summary_words=100, language=\"Spanish\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output only contains the translated summary. We can use the `.batch()` method to get the summarized blog post as well.\n",
    "\n",
    "Notice that I adjusted the prompt to return the summary in the original language. Without this instruction, the LLM would return the summary in the language of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarized Blog Post: Der Blogbeitrag beschreibt einen dreitägigen Aufenthalt in Melbourne, der als \"lebenswerteste Stadt der Welt\" gilt. Die Autoren erkunden die Stadt, die für ihre beeindruckende Architektur, Kunst, Kultur und Gastronomie bekannt ist. Sie besuchen die Flinders Street Station, die Royal Arcade und erleben das Tennisfieber während der Australian Open. Streetart in Gassen wie Hosier Lane wird hervorgehoben, ebenso wie ein Besuch im ACMI-Film Museum, wo sie mehr über Aardman-Produktionen erfahren. Zudem lernen sie die Legende von Ned Kelly in der Victoria State Library kennen. Insgesamt genießen sie ihre Zeit in Melbourne und danken ihren Gastgebern. \n",
      "\n",
      "Translated Blog Post Summary: El blog describe una estancia de tres días en Melbourne, que es considerada la \"ciudad más habitable del mundo\". Los autores exploran la ciudad, conocida por su impresionante arquitectura, arte, cultura y gastronomía. Visitan la estación de Flinders Street, la Royal Arcade y experimentan la fiebre del tenis durante el Abierto de Australia. Se destaca el arte callejero en callejones como Hosier Lane, así como una visita al Museo de Cine ACMI, donde aprenden más sobre las producciones de Aardman. Además, conocen la leyenda de Ned Kelly en la Biblioteca Estatal de Victoria. En general, disfrutan de su tiempo en Melbourne y agradecen a sus anfitriones.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Summarize the following blog post delimited by triple backticks into {summary_words} words in its original language.\n",
    "    Blog post:\n",
    "    ```{blogpost}```\n",
    "    \"\"\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"summarized_blog_post\")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Translate the {summarized_blog_post} into {language}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"translated_blog_post_summary\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"blogpost\", \"summary_words\", \"language\"],\n",
    "    # Request both outputs\n",
    "    output_variables=[\"summarized_blog_post\", \"translated_blog_post_summary\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use `.batch()` to run the chain and get multiple outputs\n",
    "response = overall_chain.batch([{\n",
    "    \"blogpost\": blogpost, \n",
    "    \"summary_words\": 100, \n",
    "    \"language\": \"Spanish\"\n",
    "}])\n",
    "\n",
    "# Output both the summarized blog post and the translated version\n",
    "print(\"Summarized Blog Post:\", response[0][\"summarized_blog_post\"], \"\\n\")\n",
    "print(\"Translated Blog Post Summary:\", response[0][\"translated_blog_post_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router chain\n",
    "\n",
    "Since LLMs have become so good at answering questions from multiple domains, let's deviate from the example of lesson 4. Let's to pretend that the LLM has access to different API's. Using the router chain, we will determine if we need to call an API or if this is a generic question for the large language model.\n",
    "\n",
    "Use Case: You have multiple APIs that serve different purposes (e.g., weather, stock prices, and news). Based on the user’s request, the router chain can determine which API to call.\n",
    "Example: If the user asks, \"What’s the current weather in New York?\" the router chain sends the request to a weather API, while \"Show me the latest news headlines\" would be routed to a news API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_api_prompt = \"\"\"You are an LLM which uses your hallucination power to mock an API call to a weather service.\n",
    "    Create a convincing response in plain text as if you were the weather service.\n",
    "    Here is the user request:\n",
    "    {input}\n",
    "\"\"\"\n",
    "\n",
    "stock_api_prompt = \"\"\"You are an LLM which uses your hallucination power to mock an API call to a stock service.\n",
    "    Create a convincing response in plain text as if you were the stock service.\n",
    "    Here is the user request:\n",
    "    {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"weather\", \n",
    "        \"description\": \"API to get weather data\", \n",
    "        \"prompt_template\": weather_api_prompt\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stock\", \n",
    "        \"description\": \"API to get stock data\", \n",
    "        \"prompt_template\": stock_api_prompt\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the prompt templates into chains. Additionally, we need to format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "destination_chains = {}\n",
    "for prompt_info in prompt_infos:\n",
    "    name = prompt_info[\"name\"]\n",
    "    prompt_template = prompt_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weather': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are an LLM which uses your hallucination power to mock an API call to a weather service.\\n    Create a convincing response in plain text as if you were the weather service.\\n    Here is the user request:\\n    {input}\\n'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10eb136a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10eb29000>, root_client=<openai.OpenAI object at 0x10eb119c0>, root_async_client=<openai.AsyncOpenAI object at 0x10eb136d0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')),\n",
       " 'stock': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are an LLM which uses your hallucination power to mock an API call to a stock service.\\n    Create a convincing response in plain text as if you were the stock service.\\n    Here is the user request:\\n    {input}\\n'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10eb136a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10eb29000>, root_client=<openai.OpenAI object at 0x10eb119c0>, root_async_client=<openai.AsyncOpenAI object at 0x10eb136d0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''))}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather: API to get weather data\n",
      "stock: API to get stock data\n"
     ]
    }
   ],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model, select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrwittm/miniforge3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "weather: {'input': 'What is the weather in New York?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Weather Report for New York City**\n",
      "\n",
      "**Date:** October 5, 2023  \n",
      "**Time:** 10:00 AM EDT\n",
      "\n",
      "**Current Conditions:**  \n",
      "- **Temperature:** 68°F (20°C)  \n",
      "- **Humidity:** 60%  \n",
      "- **Wind:** 10 mph from the NW  \n",
      "- **Conditions:** Partly cloudy with occasional sunshine\n",
      "\n",
      "**Forecast:**  \n",
      "- **Today:** Expect a high of 72°F (22°C) with a mix of sun and clouds. A slight chance of isolated showers in the late afternoon.  \n",
      "- **Tonight:** Temperatures will drop to around 58°F (14°C) under mostly clear skies.\n",
      "\n",
      "**Additional Information:**  \n",
      "- **Sunrise:** 6:45 AM  \n",
      "- **Sunset:** 6:30 PM  \n",
      "- **UV Index:** Moderate (5)\n",
      "\n",
      "Stay tuned for updates, and enjoy your day in New York!\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is the weather in New York?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "stock: {'input': 'What is the current stock price of Nvidia?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As of the latest update, the current stock price of Nvidia (NVDA) is $450.25. Please note that stock prices are subject to change and may vary throughout the trading day. For the most accurate and up-to-date information, please check a reliable financial news source or stock market platform.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is the stock price of Nvidia?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical object that absorbs all incident radiation, regardless of frequency or angle of incidence. A black body is also a perfect emitter of radiation, meaning it emits energy at all wavelengths.\n",
      "\n",
      "The characteristics of black body radiation are described by Planck's law, which states that the intensity of radiation emitted by a black body at a given temperature is a function of wavelength. Key points about black body radiation include:\n",
      "\n",
      "1. **Temperature Dependence**: The amount and spectrum of radiation emitted by a black body depend on its temperature. As the temperature increases, the total energy emitted increases, and the peak wavelength of the emitted radiation shifts to shorter wavelengths (Wien's displacement law).\n",
      "\n",
      "2. **Spectrum**: The radiation emitted by a black body is continuous and covers a wide range of wavelengths, from infrared to visible light and beyond. At lower temperatures, the radiation is primarily in the infrared range, while at higher temperatures, it can include visible light.\n",
      "\n",
      "3. **Stefan-Boltzmann Law**: The total energy emitted per unit surface area of a black body is proportional to the fourth power of its absolute temperature (T). This is expressed mathematically as \\( E = \\sigma T^4 \\), where \\( E \\) is the total energy emitted, \\( T \\) is the temperature in Kelvin, and \\( \\sigma \\) is the Stefan-Boltzmann constant.\n",
      "\n",
      "4. **Quantum Mechanics**: The study of black body radiation played a crucial role in the development of quantum mechanics. The ultraviolet catastrophe, which arose from classical physics predictions that suggested an infinite amount of energy would be emitted at short wavelengths, was resolved by Max Planck's introduction of quantized energy levels, leading to the formulation of Planck's law.\n",
      "\n",
      "Black body radiation is fundamental in fields such as thermodynamics, astrophysics, and quantum mechanics, and it serves as a model for understanding the emission of radiation from real objects, which can be approximated as black bodies under certain conditions.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(\"What is black body radiation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Router Chain\n",
    "\n",
    "After having recreated the example from the lesson, I feel that the approach used in the lesson is overly complicated.\n",
    "\n",
    "Essentially, the router prompt is doing a classification task, and it should be possible to use an output parser to directly get the result and simplify the code. After all, this complicated prompt is difficult to read and difficult to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{{\n",
      "\t\"destination\": string  // name of the prompt to use or `DEFAULT`\n",
      "\t\"next_inputs\": string  // A dictionary with the key 'input' containing the modified or original user input\n",
      "}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "destination_schema = ResponseSchema(name=\"destination\", description=\"name of the prompt to use or `DEFAULT`\")\n",
    "next_inputs_schema = ResponseSchema(\n",
    "    name=\"next_inputs\", \n",
    "    description=\"A dictionary with the key 'input' containing the modified or original user input\"\n",
    ")\n",
    "response_schema = [destination_schema, next_inputs_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "formatting_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Manually escape curly braces for use in PromptTemplate\n",
    "formatting_instructions = formatting_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "\n",
    "print(formatting_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather: API to get weather data\n",
      "stock: API to get stock data\n"
     ]
    }
   ],
   "source": [
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Evaluate the following input to select the best model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a description of what the prompt is best suited for.\n",
    "\n",
    "<< FORMATTING >>\n",
    "{formatting_instructions}\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str,\n",
    "    formatting_instructions=formatting_instructions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the following input to select the best model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for.\n",
      "\n",
      "<< FORMATTING >>\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{{\n",
      "\t\"destination\": string  // name of the prompt to use or `DEFAULT`\n",
      "\t\"next_inputs\": string  // A dictionary with the key 'input' containing the modified or original user input\n",
      "}}\n",
      "```\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "weather: API to get weather data\n",
      "stock: API to get stock data\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct PromptTemplate setup\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,  # The correctly formatted template\n",
    "    input_variables=[\"input\"],  # The input from the user\n",
    "    output_parser=output_parser  # Parse the output, but destination is not an input variable\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] output_parser=StructuredOutputParser(response_schemas=[ResponseSchema(name='destination', description='name of the prompt to use or `DEFAULT`', type='string'), ResponseSchema(name='next_inputs', description=\"A dictionary with the key 'input' containing the modified or original user input\", type='string')]) template='Evaluate the following input to select the best model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for.\\n\\n<< FORMATTING >>\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{{\\n\\t\"destination\": string  // name of the prompt to use or `DEFAULT`\\n\\t\"next_inputs\": string  // A dictionary with the key \\'input\\' containing the modified or original user input\\n}}\\n```\\n\\n<< CANDIDATE PROMPTS >>\\nweather: API to get weather data\\nstock: API to get stock data\\n\\n<< INPUT >>\\n{input}'\n"
     ]
    }
   ],
   "source": [
    "router_prompt.input_variables=['input']\n",
    "print(router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "weather: {'input': 'What is the weather in New York?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Weather Report for New York City**\n",
      "\n",
      "**Date:** October 5, 2023  \n",
      "**Time:** 10:00 AM EDT\n",
      "\n",
      "**Current Conditions:**  \n",
      "- **Temperature:** 68°F (20°C)  \n",
      "- **Humidity:** 60%  \n",
      "- **Wind:** 10 mph from the NW  \n",
      "- **Conditions:** Partly cloudy with occasional sunshine  \n",
      "\n",
      "**Forecast:**  \n",
      "- **Today:** Expect a high of 72°F (22°C) with a mix of sun and clouds. A light breeze will make it feel comfortable throughout the day.  \n",
      "- **Tonight:** Temperatures will drop to around 58°F (14°C) under mostly clear skies.  \n",
      "\n",
      "**Extended Forecast:**  \n",
      "- **Saturday:** Mostly sunny with a high of 75°F (24°C).  \n",
      "- **Sunday:** Chance of scattered showers in the afternoon, high near 70°F (21°C).  \n",
      "\n",
      "**Advisory:** No significant weather advisories are in effect. Enjoy your day!\n",
      "\n",
      "For more updates, visit our website or check your local news station.\n"
     ]
    }
   ],
   "source": [
    "response = chain.run(input=\"What is the weather in New York?\", destinations=destinations_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
